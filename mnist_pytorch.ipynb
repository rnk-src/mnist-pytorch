{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Convolutional Neural Network trained on MNIST in PyTorch\n",
    "\n",
    "In this notebook, we aim to use a convolutional neural network to classify handwritten digits from 0-9 on 28x28 images in the MNIST dataset. There are 4 parts to this notebook:\n",
    "\n",
    "-Preparing the dataset\n",
    "-Creating the CNN\n",
    "-Training the CNN\n",
    "-Results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preparing the dataset\n",
    "\n",
    "We will first get the MNIST dataset through <code>torchvision.datasets</code>, and as the train and test sets are already split up, we can create DataLoader for them. In order to improve the efficiency of gradient descent, we will also normalize our dataset on a Gaussian distribution using given means and standard deviations. We also set up the device and our hyperparameters for the upcoming model. In order for this progress to be replicable, we use a seed. In order to store our runs and checkpoints, we store each run as a folder containing checkpoints from each epoch."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "now = datetime.now()\n",
    "now = now.strftime(\"%b-%d-%Y %H:%M:%S\")\n",
    "os.mkdir()\n",
    "\n",
    "seed = 100\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "mnist_train = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,),(0.3801,))]))\n",
    "mnist_train = data.DataLoader(dataset=mnist_train, batch_size=16, shuffle=True, num_workers=8)\n",
    "\n",
    "mnist_test = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,),(0.3801,))]))\n",
    "mnist_test = data.DataLoader(dataset=mnist_test, batch_size=16, shuffle=True, num_workers=8)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "learning_rate = 1e-3\n",
    "epochs = 100"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating the CNN\n",
    "\n",
    "Now we can focus on our CNN architecture. Taking inspiration from the organized structure of VGG16/19, we will use a common kernel size. We will keep the same height and width between convolutions by using a stride and padding of 1, and we will half the height and width by using a kernel size of 2 in our pooling layers. We repeat the process of 2 convolutional layers and a max pooling layer twice, and finish with 3 fully connected layers. Weight initialization is already based on He-et-al initialization and does not need to be implemented separately. Although tuned to work better with LeakyReLU, there is insignificant performance differences when using ReLU instead."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MNISTNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTNet, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        # Starting with 1x28x28\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=3, padding=1, stride=1) # 4x28x28\n",
    "        self.conv2 = nn.Conv2d(in_channels=4, out_channels=8, kernel_size=3, padding=1, stride=1) # 8x28x28\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2) # 8x14x14\n",
    "        self.conv3 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, padding=1, stride=1) # 16x14x14\n",
    "        self.conv4 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1, stride=1) ## 32x14x14\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2) ## 32x7x7\n",
    "        self.fc1 = nn.Linear(in_features=32 * 7 * 7, out_features= 1024)\n",
    "        self.fc2 = nn.Linear(in_features=1024, out_features=256)\n",
    "        self.fc3 = nn.Linear(in_features=256, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.relu(self.conv4(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = MNISTNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "train_loss_array = []\n",
    "test_loss_array = []\n",
    "epoch_array = []\n",
    "total_loss = 0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training the CNN\n",
    "\n",
    "Now that we have created our model, we will focus on the training and validation loops. Let's first start with the training loop. We also create checkpoints during each epoch."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    for inputs, labels in mnist_train:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output, labels)\n",
    "        print(\"Training - Epoch: \" + str(epoch) + \" Loss: \" + str(loss.item()))\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Total Train loss on epoch \" + str(epoch) + \": \" + str(total_loss))\n",
    "    train_loss_array.append(total_loss)\n",
    "    total_loss = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in mnist_test:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(inputs)\n",
    "            loss = criterion(output, labels)\n",
    "            print(\"Testing - Epoch: \" + str(epoch) + \" Loss: \" + str(loss.item()))\n",
    "            total_loss += loss.item()\n",
    "    print(\"Total Test loss on epoch \" + str(epoch) + \": \" + str(total_loss))\n",
    "    test_loss_array.append(total_loss)\n",
    "    total_loss = 0\n",
    "    epoch_array.append(epoch)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
