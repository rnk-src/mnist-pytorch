{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Convolutional Neural Network trained on MNIST in PyTorch\n",
    "\n",
    "In this notebook, we aim to use a convolutional neural network to classify handwritten digits from 0-9 on 28x28 images in the MNIST dataset using PyTorch 1.12.1. There are 4 parts to this notebook:\n",
    "\n",
    "-Preparing the dataset and other hyperparameters\n",
    "-Creating the CNN\n",
    "-Training the CNN\n",
    "-Results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preparing the dataset and other hyperparameters\n",
    "\n",
    "Before we prepare the dataset, we first create a new directory to store this run's checkpoints in, and we set a manual seed in order to have replicable results. We use the MNIST dataset provided in the <code>torchvision</code> library to access the MNIST dataset, we set the correct train parameter (True for the training set, False for the test set), and we set the transformations, using <code>ToTensor</code> in order to be processed and <code>Normalize</code> to allow gradient descent to perform more efficiently on a Gaussian distribution. We then create our dataloaders, which are used to efficiently load data using parallel computing and yields quicker results. We set up our dataset, our batch size (which we set to 64 as these are relatively small images), and shuffle to True in order to introduce more randomness. In order to follow the results of our model through various epochs, we will use subplots to measure training/validation loss and accuracy along every epoch. We finish by setting up the device (to use cuda if available to speed up the program) and hyperparameters such as learning rate and epochs."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 2 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi0AAAGECAYAAAD6EhDJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAwqUlEQVR4nO3df2yV5f3/8dehP85Bs3MU0FJsqcUgUIhYTqU/SDFOLIK/yGcbTbZPJwZ1zVz40bhJxYmwfdKwTT5QBRym2pBp6UcLQrIyqYmUIg0L3SlZhpsobK3sdKRoe0BnK3B9/yCc746nVO5jf3Adno/k/uO+eN8X79vLer+8zo+6jDFGAAAAV7gRw90AAADA5SC0AAAAKxBaAACAFQgtAADACoQWAABgBUILAACwAqEFAABYgdACAACsQGgBAABWILQAAAArOA4t+/bt0wMPPKBx48bJ5XLprbfe+tprGhsb5ff75fF4NGHCBL300ktRNXV1dcrKypLb7VZWVpZ27NjhtDUAABDHHIeWzz77TNOnT9eLL754WfXHjx/X/PnzVVhYqEAgoKefflpLlixRXV1duKa5uVnFxcUqKSnR4cOHVVJSooULF+rgwYNO2wMAAHHK9U1+YaLL5dKOHTu0YMGCS9Y89dRT2rVrl95///3wWGlpqQ4fPqzm5mZJUnFxsUKhkHbv3h2uuffee3X99derpqYm1vYAAEAcSRzsv6C5uVlFRUURY3PnzlVVVZW+/PJLJSUlqbm5WcuXL4+qWb9+/SXn7enpUU9PT/j8/Pnz+uSTTzR69Gi5XK4BvQcAAOCMMUanT5/WuHHjNGLEwLyFdtBDS0dHh1JSUiLGUlJSdPbsWXV2dio1NfWSNR0dHZect6KiQqtXrx6UngEAwMBob29XWlragMw16KFFUtTOx8VXpP5zvK+a/nZMysvLVVZWFj7v7u7W+PHj1d7eLq/XOxBtAwCAGIVCIaWnp+tb3/rWgM056KFl7NixUTsmJ0+eVGJiokaPHt1vzVd3X/6T2+2W2+2OGvd6vYQWAACuEAP5lo1B/56W/Px8NTQ0RIzt2bNHOTk5SkpK6remoKBgsNsDAACWcLzTcubMGX344Yfh8+PHj6u1tVWjRo3S+PHjVV5erhMnTmjr1q2SLnxS6MUXX1RZWZkee+wxNTc3q6qqKuJTQUuXLtXs2bO1du1aPfTQQ9q5c6feeecd7d+/fwBuEQAAxAPHOy2HDh1Sdna2srOzJUllZWXKzs7Ws88+K0kKBoNqa2sL12dmZqq+vl579+7V7bffrl/84heqrKzUd77znXBNQUGBtm3bpldffVW33XabqqurVVtbq9zc3G96fwAAIE58o+9puZKEQiH5fD51d3fznhYAAIbZYDyX+d1DAADACoQWAABgBUILAACwAqEFAABYgdACAACsQGgBAABWILQAAAArEFoAAIAVCC0AAMAKhBYAAGAFQgsAALACoQUAAFiB0AIAAKxAaAEAAFYgtAAAACsQWgAAgBUILQAAwAqEFgAAYAVCCwAAsAKhBQAAWIHQAgAArBBTaNm0aZMyMzPl8Xjk9/vV1NR0ydpFixbJ5XJFHVOnTg3XVFdX91nzxRdfxNIeAACIQ45DS21trZYtW6aVK1cqEAiosLBQ8+bNU1tbW5/1GzZsUDAYDB/t7e0aNWqUvve970XUeb3eiLpgMCiPxxPbXQEAgLiT6PSCdevWafHixXr00UclSevXr9fbb7+tzZs3q6KiIqre5/PJ5/OFz9966y19+umneuSRRyLqXC6Xxo4de9l99PT0qKenJ3weCoWc3goAALCIo52W3t5etbS0qKioKGK8qKhIBw4cuKw5qqqqNGfOHGVkZESMnzlzRhkZGUpLS9P999+vQCDQ7zwVFRXhQOTz+ZSenu7kVgAAgGUchZbOzk6dO3dOKSkpEeMpKSnq6Oj42uuDwaB2794d3qW5aPLkyaqurtauXbtUU1Mjj8ejWbNm6ejRo5ecq7y8XN3d3eGjvb3dya0AAADLOH55SLrwUs5/MsZEjfWlurpa1113nRYsWBAxnpeXp7y8vPD5rFmzNGPGDL3wwguqrKzscy632y232+28eQAAYCVHOy1jxoxRQkJC1K7KyZMno3ZfvsoYo1deeUUlJSVKTk7uv6kRI3THHXf0u9MCAACuLo5CS3Jysvx+vxoaGiLGGxoaVFBQ0O+1jY2N+vDDD7V48eKv/XuMMWptbVVqaqqT9gAAQBxz/PJQWVmZSkpKlJOTo/z8fG3ZskVtbW0qLS2VdOG9JidOnNDWrVsjrquqqlJubq6mTZsWNefq1auVl5eniRMnKhQKqbKyUq2trdq4cWOMtwUAAOKN49BSXFysU6dOac2aNQoGg5o2bZrq6+vDnwYKBoNR39nS3d2turo6bdiwoc85u7q69Pjjj6ujo0M+n0/Z2dnat2+fZs6cGcMtAQCAeOQyxpjhbmIghEIh+Xw+dXd3y+v1Dnc7AABc1QbjuczvHgIAAFYgtAAAACsQWgAAgBUILQAAwAqEFgAAYAVCCwAAsAKhBQAAWIHQAgAArEBoAQAAViC0AAAAKxBaAACAFQgtAADACoQWAABgBUILAACwAqEFAABYgdACAACsQGgBAABWILQAAAArEFoAAIAVCC0AAMAKhBYAAGCFmELLpk2blJmZKY/HI7/fr6ampkvW7t27Vy6XK+r461//GlFXV1enrKwsud1uZWVlaceOHbG0BgAA4pTj0FJbW6tly5Zp5cqVCgQCKiws1Lx589TW1tbvdX/7298UDAbDx8SJE8N/1tzcrOLiYpWUlOjw4cMqKSnRwoULdfDgQed3BAAA4pLLGGOcXJCbm6sZM2Zo8+bN4bEpU6ZowYIFqqioiKrfu3ev7rrrLn366ae67rrr+pyzuLhYoVBIu3fvDo/de++9uv7661VTU9PnNT09Perp6Qmfh0Ihpaenq7u7W16v18ktAQCAARYKheTz+Qb0uexop6W3t1ctLS0qKiqKGC8qKtKBAwf6vTY7O1upqam6++679e6770b8WXNzc9Scc+fO7XfOiooK+Xy+8JGenu7kVgAAgGUchZbOzk6dO3dOKSkpEeMpKSnq6Ojo85rU1FRt2bJFdXV12r59uyZNmqS7775b+/btC9d0dHQ4mlOSysvL1d3dHT7a29ud3AoAALBMYiwXuVyuiHNjTNTYRZMmTdKkSZPC5/n5+Wpvb9dvfvMbzZ49O6Y5JcntdsvtdsfSPgAAsJCjnZYxY8YoISEhagfk5MmTUTsl/cnLy9PRo0fD52PHjv3GcwIAgPjmKLQkJyfL7/eroaEhYryhoUEFBQWXPU8gEFBqamr4PD8/P2rOPXv2OJoTAADEN8cvD5WVlamkpEQ5OTnKz8/Xli1b1NbWptLSUkkX3mty4sQJbd26VZK0fv163XzzzZo6dap6e3v1u9/9TnV1daqrqwvPuXTpUs2ePVtr167VQw89pJ07d+qdd97R/v37B+g2AQCA7RyHluLiYp06dUpr1qxRMBjUtGnTVF9fr4yMDElSMBiM+M6W3t5ePfnkkzpx4oRGjhypqVOn6ve//73mz58frikoKNC2bdv0zDPP6Oc//7luueUW1dbWKjc3dwBuEQAAxAPH39NypRqMz4MDAIDYDPv3tAAAAAwXQgsAALACoQUAAFiB0AIAAKxAaAEAAFYgtAAAACsQWgAAgBUILQAAwAqEFgAAYAVCCwAAsAKhBQAAWIHQAgAArEBoAQAAViC0AAAAKxBaAACAFQgtAADACoQWAABgBUILAACwAqEFAABYgdACAACsQGgBAABWiCm0bNq0SZmZmfJ4PPL7/Wpqarpk7fbt23XPPffohhtukNfrVX5+vt5+++2Imurqarlcrqjjiy++iKU9AAAQhxyHltraWi1btkwrV65UIBBQYWGh5s2bp7a2tj7r9+3bp3vuuUf19fVqaWnRXXfdpQceeECBQCCizuv1KhgMRhwejye2uwIAAHHHZYwxTi7Izc3VjBkztHnz5vDYlClTtGDBAlVUVFzWHFOnTlVxcbGeffZZSRd2WpYtW6aurq7L7qOnp0c9PT3h81AopPT0dHV3d8vr9V72PAAAYOCFQiH5fL4BfS472mnp7e1VS0uLioqKIsaLiop04MCBy5rj/PnzOn36tEaNGhUxfubMGWVkZCgtLU33339/1E7MV1VUVMjn84WP9PR0J7cCAAAs4yi0dHZ26ty5c0pJSYkYT0lJUUdHx2XN8fzzz+uzzz7TwoULw2OTJ09WdXW1du3apZqaGnk8Hs2aNUtHjx695Dzl5eXq7u4OH+3t7U5uBQAAWCYxlotcLlfEuTEmaqwvNTU1eu6557Rz507deOON4fG8vDzl5eWFz2fNmqUZM2bohRdeUGVlZZ9zud1uud3uWNoHAAAWchRaxowZo4SEhKhdlZMnT0btvnxVbW2tFi9erDfeeENz5szpt3bEiBG64447+t1pAQAAVxdHLw8lJyfL7/eroaEhYryhoUEFBQWXvK6mpkaLFi3S66+/rvvuu+9r/x5jjFpbW5WamuqkPQAAEMccvzxUVlamkpIS5eTkKD8/X1u2bFFbW5tKS0slXXivyYkTJ7R161ZJFwLLD3/4Q23YsEF5eXnhXZqRI0fK5/NJklavXq28vDxNnDhRoVBIlZWVam1t1caNGwfqPgEAgOUch5bi4mKdOnVKa9asUTAY1LRp01RfX6+MjAxJUjAYjPjOlt/+9rc6e/asnnjiCT3xxBPh8YcffljV1dWSpK6uLj3++OPq6OiQz+dTdna29u3bp5kzZ37D2wMAAPHC8fe0XKkG4/PgAAAgNsP+PS0AAADDhdACAACsQGgBAABWILQAAAArEFoAAIAVCC0AAMAKhBYAAGAFQgsAALACoQUAAFiB0AIAAKxAaAEAAFYgtAAAACsQWgAAgBUILQAAwAqEFgAAYAVCCwAAsAKhBQAAWIHQAgAArEBoAQAAViC0AAAAK8QUWjZt2qTMzEx5PB75/X41NTX1W9/Y2Ci/3y+Px6MJEybopZdeiqqpq6tTVlaW3G63srKytGPHjlhaAwAAccpxaKmtrdWyZcu0cuVKBQIBFRYWat68eWpra+uz/vjx45o/f74KCwsVCAT09NNPa8mSJaqrqwvXNDc3q7i4WCUlJTp8+LBKSkq0cOFCHTx4MPY7AwAAccVljDFOLsjNzdWMGTO0efPm8NiUKVO0YMECVVRURNU/9dRT2rVrl95///3wWGlpqQ4fPqzm5mZJUnFxsUKhkHbv3h2uuffee3X99derpqbmsvoKhULy+Xzq7u6W1+t1cksAAGCADcZzOdFJcW9vr1paWrRixYqI8aKiIh04cKDPa5qbm1VUVBQxNnfuXFVVVenLL79UUlKSmpubtXz58qia9evXX7KXnp4e9fT0hM+7u7slXfiHBAAAhtfF57HDvZF+OQotnZ2dOnfunFJSUiLGU1JS1NHR0ec1HR0dfdafPXtWnZ2dSk1NvWTNpeaUpIqKCq1evTpqPD09/XJvBwAADLJTp07J5/MNyFyOQstFLpcr4twYEzX2dfVfHXc6Z3l5ucrKysLnXV1dysjIUFtb24D9w4EzoVBI6enpam9v5yW6YcIaDD/W4MrAOgy/7u5ujR8/XqNGjRqwOR2FljFjxighISFqB+TkyZNROyUXjR07ts/6xMREjR49ut+aS80pSW63W263O2rc5/PxL+gw83q9rMEwYw2GH2twZWAdht+IEQP37SqOZkpOTpbf71dDQ0PEeENDgwoKCvq8Jj8/P6p+z549ysnJUVJSUr81l5oTAABcfRy/PFRWVqaSkhLl5OQoPz9fW7ZsUVtbm0pLSyVdeNnmxIkT2rp1q6QLnxR68cUXVVZWpscee0zNzc2qqqqK+FTQ0qVLNXv2bK1du1YPPfSQdu7cqXfeeUf79+8foNsEAAC2cxxaiouLderUKa1Zs0bBYFDTpk1TfX29MjIyJEnBYDDiO1syMzNVX1+v5cuXa+PGjRo3bpwqKyv1ne98J1xTUFCgbdu26ZlnntHPf/5z3XLLLaqtrVVubu5l9+V2u7Vq1ao+XzLC0GANhh9rMPxYgysD6zD8BmMNHH9PCwAAwHDgdw8BAAArEFoAAIAVCC0AAMAKhBYAAGAFQgsAALCCVaFl06ZNyszMlMfjkd/vV1NTU7/1jY2N8vv98ng8mjBhgl566aUh6jR+OVmD7du365577tENN9wgr9er/Px8vf3220PYbXxy+nNw0XvvvafExETdfvvtg9vgVcDpGvT09GjlypXKyMiQ2+3WLbfcoldeeWWIuo1PTtfgtdde0/Tp03XNNdcoNTVVjzzyiE6dOjVE3cafffv26YEHHtC4cePkcrn01ltvfe01A/JMNpbYtm2bSUpKMi+//LI5cuSIWbp0qbn22mvNP/7xjz7rjx07Zq655hqzdOlSc+TIEfPyyy+bpKQk8+abbw5x5/HD6RosXbrUrF271vzxj380H3zwgSkvLzdJSUnmT3/60xB3Hj+crsFFXV1dZsKECaaoqMhMnz59aJqNU7GswYMPPmhyc3NNQ0ODOX78uDl48KB57733hrDr+OJ0DZqamsyIESPMhg0bzLFjx0xTU5OZOnWqWbBgwRB3Hj/q6+vNypUrTV1dnZFkduzY0W/9QD2TrQktM2fONKWlpRFjkydPNitWrOiz/mc/+5mZPHlyxNiPfvQjk5eXN2g9xjuna9CXrKwss3r16oFu7aoR6xoUFxebZ555xqxatYrQ8g05XYPdu3cbn89nTp06NRTtXRWcrsGvf/1rM2HChIixyspKk5aWNmg9Xk0uJ7QM1DPZipeHent71dLSoqKioojxoqIiHThwoM9rmpubo+rnzp2rQ4cO6csvvxy0XuNVLGvwVefPn9fp06cH9Dd+Xk1iXYNXX31VH330kVatWjXYLca9WNZg165dysnJ0a9+9SvddNNNuvXWW/Xkk0/q3//+91C0HHdiWYOCggJ9/PHHqq+vlzFG//rXv/Tmm2/qvvvuG4qWoYF7Jjv+Gv/h0NnZqXPnzkX91ueUlJSo3w59UUdHR5/1Z8+eVWdnp1JTUwet33gUyxp81fPPP6/PPvtMCxcuHIwW414sa3D06FGtWLFCTU1NSky04sf9ihbLGhw7dkz79++Xx+PRjh071NnZqR//+Mf65JNPeF9LDGJZg4KCAr322msqLi7WF198obNnz+rBBx/UCy+8MBQtQwP3TLZip+Uil8sVcW6MiRr7uvq+xnH5nK7BRTU1NXruuedUW1urG2+8cbDauypc7hqcO3dO3//+97V69WrdeuutQ9XeVcHJz8H58+flcrn02muvaebMmZo/f77WrVun6upqdlu+ASdrcOTIES1ZskTPPvusWlpa9Ic//EHHjx8P/6JfDI2BeCZb8b9eY8aMUUJCQlSKPnnyZFRyu2js2LF91icmJmr06NGD1mu8imUNLqqtrdXixYv1xhtvaM6cOYPZZlxzuganT5/WoUOHFAgE9JOf/ETShQeoMUaJiYnas2ePvv3tbw9J7/Eilp+D1NRU3XTTTfL5fOGxKVOmyBijjz/+WBMnThzUnuNNLGtQUVGhWbNm6ac//akk6bbbbtO1116rwsJC/fKXv2TnfQgM1DPZip2W5ORk+f1+NTQ0RIw3NDSooKCgz2vy8/Oj6vfs2aOcnBwlJSUNWq/xKpY1kC7ssCxatEivv/46rx9/Q07XwOv16s9//rNaW1vDR2lpqSZNmqTW1lZHv0UdF8TyczBr1iz985//1JkzZ8JjH3zwgUaMGKG0tLRB7TcexbIGn3/+uUaMiHzcJSQkSPr//7ePwTVgz2RHb9sdRhc/4lZVVWWOHDlili1bZq699lrz97//3RhjzIoVK0xJSUm4/uLHq5YvX26OHDliqqqq+MjzN+R0DV5//XWTmJhoNm7caILBYPjo6uoarluwntM1+Co+PfTNOV2D06dPm7S0NPPd737X/OUvfzGNjY1m4sSJ5tFHHx2uW7Ce0zV49dVXTWJiotm0aZP56KOPzP79+01OTo6ZOXPmcN2C9U6fPm0CgYAJBAJGklm3bp0JBALhj50P1jPZmtBijDEbN240GRkZJjk52cyYMcM0NjaG/+zhhx82d955Z0T93r17TXZ2tklOTjY333yz2bx58xB3HH+crMGdd95pJEUdDz/88NA3Hkec/hz8J0LLwHC6Bu+//76ZM2eOGTlypElLSzNlZWXm888/H+Ku44vTNaisrDRZWVlm5MiRJjU11fzgBz8wH3/88RB3HT/efffdfv/7PljPZJcx7I0BAIArnxXvaQEAACC0AAAAKxBaAACAFQgtAADACoQWAABgBUILAACwAqEFAABYgdACAACsQGgBAABWILQAAAArEFoAAIAVCC0AAMAKhBYAAGAFQgsAALACoQUAAFiB0AIAAKxAaAEAAFYgtAAAACsQWgAAgBUILQAAwAqEFgAAYAVCCwAAsAKhBQAAWIHQAgAArEBoAQAAViC0AAAAKxBaAACAFQgtAADACoQWAABgBUILAACwAqEFAABYgdACAACsQGgBAABWILQAAAArEFoAAIAVCC0AAMAKhBYAAGAFQgsAALACoQUAAFiB0AIAAKxAaAEAAFYgtAAAACsQWgAAgBUILQAAwAqEFgAAYAVCCwAAsAKhBQAAWIHQAgAArEBoAQAAViC0AAAAKxBaAACAFQgtAADACoQWAABgBUILAACwAqEFAABYgdACAACsQGgBAABWILQAAAArOA4t+/bt0wMPPKBx48bJ5XLprbfe+tprGhsb5ff75fF4NGHCBL300ktRNXV1dcrKypLb7VZWVpZ27NjhtDUAABDHHIeWzz77TNOnT9eLL754WfXHjx/X/PnzVVhYqEAgoKefflpLlixRXV1duKa5uVnFxcUqKSnR4cOHVVJSooULF+rgwYNO2wMAAHHKZYwxMV/scmnHjh1asGDBJWueeuop7dq1S++//354rLS0VIcPH1Zzc7Mkqbi4WKFQSLt37w7X3Hvvvbr++utVU1MTa3sAACCOJA72X9Dc3KyioqKIsblz56qqqkpffvmlkpKS1NzcrOXLl0fVrF+//pLz9vT0qKenJ3x+/vx5ffLJJxo9erRcLteA3gMAAHDGGKPTp09r3LhxGjFiYN5CO+ihpaOjQykpKRFjKSkpOnv2rDo7O5WamnrJmo6OjkvOW1FRodWrVw9KzwAAYGC0t7crLS1tQOYa9NAiKWrn4+IrUv853ldNfzsm5eXlKisrC593d3dr/Pjxam9vl9frHYi2AQBAjEKhkNLT0/Wtb31rwOYc9NAyduzYqB2TkydPKjExUaNHj+635qu7L//J7XbL7XZHjXu9XkILAABXiIF8y8agf09Lfn6+GhoaIsb27NmjnJwcJSUl9VtTUFAw2O0BAABLON5pOXPmjD788MPw+fHjx9Xa2qpRo0Zp/PjxKi8v14kTJ7R161ZJFz4p9OKLL6qsrEyPPfaYmpubVVVVFfGpoKVLl2r27Nlau3atHnroIe3cuVPvvPOO9u/fPwC3CAAA4oHjnZZDhw4pOztb2dnZkqSysjJlZ2fr2WeflSQFg0G1tbWF6zMzM1VfX6+9e/fq9ttv1y9+8QtVVlbqO9/5TrimoKBA27Zt06uvvqrbbrtN1dXVqq2tVW5u7je9PwAAECe+0fe0XElCoZB8Pp+6u7t5TwsAAMNsMJ7L/O4hAABgBUILAACwAqEFAABYgdACAACsQGgBAABWILQAAAArEFoAAIAVCC0AAMAKhBYAAGAFQgsAALACoQUAAFiB0AIAAKxAaAEAAFYgtAAAACsQWgAAgBUILQAAwAqEFgAAYAVCCwAAsAKhBQAAWIHQAgAArEBoAQAAVogptGzatEmZmZnyeDzy+/1qamq6ZO2iRYvkcrmijqlTp4Zrqqur+6z54osvYmkPAADEIcehpba2VsuWLdPKlSsVCARUWFioefPmqa2trc/6DRs2KBgMho/29naNGjVK3/ve9yLqvF5vRF0wGJTH44ntrgAAQNxxHFrWrVunxYsX69FHH9WUKVO0fv16paena/PmzX3W+3w+jR07NnwcOnRIn376qR555JGIOpfLFVE3duzY2O4IAADEJUehpbe3Vy0tLSoqKooYLyoq0oEDBy5rjqqqKs2ZM0cZGRkR42fOnFFGRobS0tJ0//33KxAI9DtPT0+PQqFQxAEAAOKXo9DS2dmpc+fOKSUlJWI8JSVFHR0dX3t9MBjU7t279eijj0aMT548WdXV1dq1a5dqamrk8Xg0a9YsHT169JJzVVRUyOfzhY/09HQntwIAACwT0xtxXS5XxLkxJmqsL9XV1bruuuu0YMGCiPG8vDz993//t6ZPn67CwkL93//9n2699Va98MILl5yrvLxc3d3d4aO9vT2WWwEAAJZIdFI8ZswYJSQkRO2qnDx5Mmr35auMMXrllVdUUlKi5OTkfmtHjBihO+64o9+dFrfbLbfbffnNAwAAqznaaUlOTpbf71dDQ0PEeENDgwoKCvq9trGxUR9++KEWL178tX+PMUatra1KTU110h4AAIhjjnZaJKmsrEwlJSXKyclRfn6+tmzZora2NpWWlkq68LLNiRMntHXr1ojrqqqqlJubq2nTpkXNuXr1auXl5WnixIkKhUKqrKxUa2urNm7cGONtAQCAeOM4tBQXF+vUqVNas2aNgsGgpk2bpvr6+vCngYLBYNR3tnR3d6uurk4bNmzoc86uri49/vjj6ujokM/nU3Z2tvbt26eZM2fGcEsAACAeuYwxZribGAihUEg+n0/d3d3yer3D3Q4AAFe1wXgu87uHAACAFQgtAADACoQWAABgBUILAACwAqEFAABYgdACAACsQGgBAABWILQAAAArEFoAAIAVCC0AAMAKhBYAAGAFQgsAALACoQUAAFiB0AIAAKxAaAEAAFYgtAAAACsQWgAAgBUILQAAwAqEFgAAYAVCCwAAsAKhBQAAWCGm0LJp0yZlZmbK4/HI7/erqanpkrV79+6Vy+WKOv76179G1NXV1SkrK0tut1tZWVnasWNHLK0BAIA45Ti01NbWatmyZVq5cqUCgYAKCws1b948tbW19Xvd3/72NwWDwfAxceLE8J81NzeruLhYJSUlOnz4sEpKSrRw4UIdPHjQ+R0BAIC45DLGGCcX5ObmasaMGdq8eXN4bMqUKVqwYIEqKiqi6vfu3au77rpLn376qa677ro+5ywuLlYoFNLu3bvDY/fee6+uv/561dTUXFZfoVBIPp9P3d3d8nq9Tm4JAAAMsMF4Ljvaaent7VVLS4uKiooixouKinTgwIF+r83OzlZqaqruvvtuvfvuuxF/1tzcHDXn3Llz+52zp6dHoVAo4gAAAPHLUWjp7OzUuXPnlJKSEjGekpKijo6OPq9JTU3Vli1bVFdXp+3bt2vSpEm6++67tW/fvnBNR0eHozklqaKiQj6fL3ykp6c7uRUAAGCZxFgucrlcEefGmKixiyZNmqRJkyaFz/Pz89Xe3q7f/OY3mj17dkxzSlJ5ebnKysrC56FQiOACAEAcc7TTMmbMGCUkJETtgJw8eTJqp6Q/eXl5Onr0aPh87Nixjud0u93yer0RBwAAiF+OQktycrL8fr8aGhoixhsaGlRQUHDZ8wQCAaWmpobP8/Pzo+bcs2ePozkBAEB8c/zyUFlZmUpKSpSTk6P8/Hxt2bJFbW1tKi0tlXThZZsTJ05o69atkqT169fr5ptv1tSpU9Xb26vf/e53qqurU11dXXjOpUuXavbs2Vq7dq0eeugh7dy5U++88472798/QLcJAABs5zi0FBcX69SpU1qzZo2CwaCmTZum+vp6ZWRkSJKCwWDEd7b09vbqySef1IkTJzRy5EhNnTpVv//97zV//vxwTUFBgbZt26ZnnnlGP//5z3XLLbeotrZWubm5A3CLAAAgHjj+npYrFd/TAgDAlWPYv6cFAABguBBaAACAFQgtAADACoQWAABgBUILAACwAqEFAABYgdACAACsQGgBAABWILQAAAArEFoAAIAVCC0AAMAKhBYAAGAFQgsAALACoQUAAFiB0AIAAKxAaAEAAFYgtAAAACsQWgAAgBUILQAAwAqEFgAAYAVCCwAAsEJMoWXTpk3KzMyUx+OR3+9XU1PTJWu3b9+ue+65RzfccIO8Xq/y8/P19ttvR9RUV1fL5XJFHV988UUs7QEAgDjkOLTU1tZq2bJlWrlypQKBgAoLCzVv3jy1tbX1Wb9v3z7dc889qq+vV0tLi+666y498MADCgQCEXVer1fBYDDi8Hg8sd0VAACIOy5jjHFyQW5urmbMmKHNmzeHx6ZMmaIFCxaooqLisuaYOnWqiouL9eyzz0q6sNOybNkydXV1XXYfPT096unpCZ+HQiGlp6eru7tbXq/3sucBAAADLxQKyefzDehz2dFOS29vr1paWlRUVBQxXlRUpAMHDlzWHOfPn9fp06c1atSoiPEzZ84oIyNDaWlpuv/++6N2Yr6qoqJCPp8vfKSnpzu5FQAAYBlHoaWzs1Pnzp1TSkpKxHhKSoo6Ojoua47nn39en332mRYuXBgemzx5sqqrq7Vr1y7V1NTI4/Fo1qxZOnr06CXnKS8vV3d3d/hob293cisAAMAyibFc5HK5Is6NMVFjfampqdFzzz2nnTt36sYbbwyP5+XlKS8vL3w+a9YszZgxQy+88IIqKyv7nMvtdsvtdsfSPgAAsJCj0DJmzBglJCRE7aqcPHkyavflq2pra7V48WK98cYbmjNnTr+1I0aM0B133NHvTgsAALi6OHp5KDk5WX6/Xw0NDRHjDQ0NKigouOR1NTU1WrRokV5//XXdd999X/v3GGPU2tqq1NRUJ+0BAIA45vjlobKyMpWUlCgnJ0f5+fnasmWL2traVFpaKunCe01OnDihrVu3SroQWH74wx9qw4YNysvLC+/SjBw5Uj6fT5K0evVq5eXlaeLEiQqFQqqsrFRra6s2btw4UPcJAAAs5zi0FBcX69SpU1qzZo2CwaCmTZum+vp6ZWRkSJKCwWDEd7b89re/1dmzZ/XEE0/oiSeeCI8//PDDqq6uliR1dXXp8ccfV0dHh3w+n7Kzs7Vv3z7NnDnzG94eAACIF46/p+VKNRifBwcAALEZ9u9pAQAAGC6EFgAAYAVCCwAAsAKhBQAAWIHQAgAArEBoAQAAViC0AAAAKxBaAACAFQgtAADACoQWAABgBUILAACwAqEFAABYgdACAACsQGgBAABWILQAAAArEFoAAIAVCC0AAMAKhBYAAGAFQgsAALACoQUAAFiB0AIAAKwQU2jZtGmTMjMz5fF45Pf71dTU1G99Y2Oj/H6/PB6PJkyYoJdeeimqpq6uTllZWXK73crKytKOHTtiaQ0AAMQpx6GltrZWy5Yt08qVKxUIBFRYWKh58+apra2tz/rjx49r/vz5KiwsVCAQ0NNPP60lS5aorq4uXNPc3Kzi4mKVlJTo8OHDKikp0cKFC3Xw4MHY7wwAAMQVlzHGOLkgNzdXM2bM0ObNm8NjU6ZM0YIFC1RRURFV/9RTT2nXrl16//33w2OlpaU6fPiwmpubJUnFxcUKhULavXt3uObee+/V9ddfr5qamj776OnpUU9PT/i8u7tb48ePV3t7u7xer5NbAgAAAywUCik9PV1dXV3y+XwDM6lxoKenxyQkJJjt27dHjC9ZssTMnj27z2sKCwvNkiVLIsa2b99uEhMTTW9vrzHGmPT0dLNu3bqImnXr1pnx48dfspdVq1YZSRwcHBwcHBxX8PHRRx85iRr9SpQDnZ2dOnfunFJSUiLGU1JS1NHR0ec1HR0dfdafPXtWnZ2dSk1NvWTNpeaUpPLycpWVlYXPu7q6lJGRoba2toFLdHDkYqpmt2v4sAbDjzW4MrAOw+/iKyCjRo0asDkdhZaLXC5XxLkxJmrs6+q/Ou50TrfbLbfbHTXu8/n4F3SYeb1e1mCYsQbDjzW4MrAOw2/EiIH7oLKjmcaMGaOEhISoHZCTJ09G7ZRcNHbs2D7rExMTNXr06H5rLjUnAAC4+jgKLcnJyfL7/WpoaIgYb2hoUEFBQZ/X5OfnR9Xv2bNHOTk5SkpK6rfmUnMCAICrj+OXh8rKylRSUqKcnBzl5+dry5YtamtrU2lpqaQL7zU5ceKEtm7dKunCJ4VefPFFlZWV6bHHHlNzc7OqqqoiPhW0dOlSzZ49W2vXrtVDDz2knTt36p133tH+/fsvuy+3261Vq1b1+ZIRhgZrMPxYg+HHGlwZWIfhNxhr4Pgjz9KFL5f71a9+pWAwqGnTpul///d/NXv2bEnSokWL9Pe//1179+4N1zc2Nmr58uX6y1/+onHjxumpp54Kh5yL3nzzTT3zzDM6duyYbrnlFv3P//yP/uu//uub3R0AAIgbMYUWAACAocbvHgIAAFYgtAAAACsQWgAAgBUILQAAwApWhZZNmzYpMzNTHo9Hfr9fTU1N/dY3NjbK7/fL4/FowoQJeumll4ao0/jlZA22b9+ue+65RzfccIO8Xq/y8/P19ttvD2G38cnpz8FF7733nhITE3X77bcPboNXAadr0NPTo5UrVyojI0Nut1u33HKLXnnllSHqNj45XYPXXntN06dP1zXXXKPU1FQ98sgjOnXq1BB1G3/27dunBx54QOPGjZPL5dJbb731tdcMyDN5wH6L0SDbtm2bSUpKMi+//LI5cuSIWbp0qbn22mvNP/7xjz7rjx07Zq655hqzdOlSc+TIEfPyyy+bpKQk8+abbw5x5/HD6RosXbrUrF271vzxj380H3zwgSkvLzdJSUnmT3/60xB3Hj+crsFFXV1dZsKECaaoqMhMnz59aJqNU7GswYMPPmhyc3NNQ0ODOX78uDl48KB57733hrDr+OJ0DZqamsyIESPMhg0bzLFjx0xTU5OZOnWqWbBgwRB3Hj/q6+vNypUrTV1dnZFkduzY0W/9QD2TrQktM2fONKWlpRFjkydPNitWrOiz/mc/+5mZPHlyxNiPfvQjk5eXN2g9xjuna9CXrKwss3r16oFu7aoR6xoUFxebZ555xqxatYrQ8g05XYPdu3cbn89nTp06NRTtXRWcrsGvf/1rM2HChIixyspKk5aWNmg9Xk0uJ7QM1DPZipeHent71dLSoqKioojxoqIiHThwoM9rmpubo+rnzp2rQ4cO6csvvxy0XuNVLGvwVefPn9fp06cH9Dd+Xk1iXYNXX31VH330kVatWjXYLca9WNZg165dysnJ0a9+9SvddNNNuvXWW/Xkk0/q3//+91C0HHdiWYOCggJ9/PHHqq+vlzFG//rXv/Tmm2/qvvvuG4qWoYF7Jsf0W56HWmdnp86dOxf1CxRTUlKiftHiRR0dHX3Wnz17Vp2dnUpNTR20fuNRLGvwVc8//7w+++wzLVy4cDBajHuxrMHRo0e1YsUKNTU1KTHRih/3K1osa3Ds2DHt379fHo9HO3bsUGdnp3784x/rk08+4X0tMYhlDQoKCvTaa6+puLhYX3zxhc6ePasHH3xQL7zwwlC0DA3cM9mKnZaLXC5XxLkxJmrs6+r7Gsflc7oGF9XU1Oi5555TbW2tbrzxxsFq76pwuWtw7tw5ff/739fq1at16623DlV7VwUnPwfnz5+Xy+XSa6+9ppkzZ2r+/Plat26dqqur2W35BpyswZEjR7RkyRI9++yzamlp0R/+8AcdP3486tfJYHANxDPZiv/1GjNmjBISEqJS9MmTJ6OS20Vjx47tsz4xMVGjR48etF7jVSxrcFFtba0WL16sN954Q3PmzBnMNuOa0zU4ffq0Dh06pEAgoJ/85CeSLjxAjTFKTEzUnj179O1vf3tIeo8XsfwcpKam6qabbpLP5wuPTZkyRcYYffzxx5o4ceKg9hxvYlmDiooKzZo1Sz/96U8lSbfddpuuvfZaFRYW6pe//CU770NgoJ7JVuy0JCcny+/3q6GhIWK8oaFBBQUFfV6Tn58fVb9nzx7l5OQoKSlp0HqNV7GsgXRhh2XRokV6/fXXef34G3K6Bl6vV3/+85/V2toaPkpLSzVp0iS1trYqNzd3qFqPG7H8HMyaNUv//Oc/debMmfDYBx98oBEjRigtLW1Q+41HsazB559/rhEjIh93CQkJkv7//+1jcA3YM9nR23aH0cWPuFVVVZkjR46YZcuWmWuvvdb8/e9/N8YYs2LFClNSUhKuv/jxquXLl5sjR46YqqoqPvL8DTldg9dff90kJiaajRs3mmAwGD66urqG6xas53QNvopPD31zTtfg9OnTJi0tzXz3u981f/nLX0xjY6OZOHGiefTRR4frFqzndA1effVVk5iYaDZt2mQ++ugjs3//fpOTk2Nmzpw5XLdgvdOnT5tAIGACgYCRZNatW2cCgUD4Y+eD9Uy2JrQYY8zGjRtNRkaGSU5ONjNmzDCNjY3hP3v44YfNnXfeGVG/d+9ek52dbZKTk83NN99sNm/ePMQdxx8na3DnnXcaSVHHww8/PPSNxxGnPwf/idAyMJyuwfvvv2/mzJljRo4cadLS0kxZWZn5/PPPh7jr+OJ0DSorK01WVpYZOXKkSU1NNT/4wQ/Mxx9/PMRdx49333233/++D9Yz2WUMe2MAAODKZ8V7WgAAAAgtAADACoQWAABgBUILAACwAqEFAABYgdACAACsQGgBAABWILQAAAArEFoAAIAVCC0AAMAKhBYAAGCF/wd0QyQ0WvvJigAAAABJRU5ErkJggg=="
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "from torchvision import datasets, transforms\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "model_load = True\n",
    "seed = 100\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "fig, [ax1, ax2] = plt.subplots(nrows=2, ncols=1)\n",
    "fig.tight_layout(pad=4.0)\n",
    "\n",
    "mnist_train = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3801,))]))\n",
    "mnist_train, mnist_validation = train_test_split(mnist_train, test_size=0.25)\n",
    "\n",
    "mnist_train = data.DataLoader(dataset=mnist_train, batch_size=64, shuffle=True, num_workers=0)\n",
    "mnist_validation = data.DataLoader(dataset=mnist_validation, batch_size=64, shuffle=True, num_workers=0)\n",
    "\n",
    "mnist_test = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3801,))]))\n",
    "mnist_test = data.DataLoader(dataset=mnist_test, batch_size=64, shuffle=True, num_workers=0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "learning_rate = 1e-3\n",
    "epochs = 10"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-21T23:55:05.230891Z",
     "start_time": "2023-07-21T23:55:02.778477Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating the CNN\n",
    "\n",
    "Now we can focus on our CNN architecture. Taking inspiration from the organized structure of VGG16/19, we will use a common kernel size. We will keep the same height and width between convolutions by using a stride and padding of 1, and we will half the height and width by using a kernel size of 2 in our pooling layers. We repeat the process of 2 convolutional layers and a max pooling layer twice, and finish with 3 fully connected layers. Weight initialization is already based on He-et-al initialization and does not need to be implemented separately. Although tuned to work better with LeakyReLU, there is insignificant performance differences when using ReLU instead. We then define functions that create folders that are named based on the time and date the program was run on in order to save checkpoints for that run in the folder, and other methods are made to load checkpoints given a relative directory. We also use <code>CrossEntropyLoss</code> as it's one of the better loss functions for classification problems, and we use Adam as it has adaptive learning rates, however as it does not change the learning rate much on its own, we also include a <code>CosineAnnealingLR</code> as it is popularly used in CV applications and gives time to the model to use a high learning rate for a couple of epochs before decreasing the LR. It is also favorable because the two hyperparameters are simple, with the number of epochs already known and simply setting the minimum LR to 0. We include a function to save checkpoints and load models in order to track and keep our progress in case something interrupts the training process. We decide to save per epoch, as each epoch takes a relatively long time to train locally. Finally, we include a function to measure accuracy for our final testing phase."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class MNISTNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTNet, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        # Starting with 1x28x28\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=3, padding=1, stride=1)  # 4x28x28\n",
    "        self.conv2 = nn.Conv2d(in_channels=4, out_channels=8, kernel_size=3, padding=1, stride=1)  # 8x28x28\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)  # 8x14x14\n",
    "        self.conv3 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, padding=1, stride=1)  # 16x14x14\n",
    "        self.conv4 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1, stride=1)  # 32x14x14\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)  # 32x7x7\n",
    "        self.fc1 = nn.Linear(in_features=32 * 7 * 7, out_features=1024)\n",
    "        self.fc2 = nn.Linear(in_features=1024, out_features=256)\n",
    "        self.fc3 = nn.Linear(in_features=256, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.relu(self.conv4(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def get_current_time():\n",
    "    now = datetime.now()\n",
    "    now = now.strftime(\"%b-%d-%Y %H:%M:%S\")\n",
    "    return now\n",
    "\n",
    "if not os.path.exists(\"runs\"):\n",
    "  os.mkdir(\"runs\")\n",
    "\n",
    "filename = \"run \" + get_current_time()\n",
    "run_directory = \"runs/\" + filename\n",
    "os.mkdir(run_directory)\n",
    "\n",
    "\n",
    "def save_checkpoint(state, directory=run_directory):\n",
    "    torch.save(state, directory + \"/checkpoint \" + get_current_time() + \".pt\")\n",
    "    print(\"saved checkpoint\")\n",
    "\n",
    "\n",
    "def get_accuracy(out, lbl):\n",
    "    n = out.size(0)\n",
    "    out = torch.softmax(out, dim=1)\n",
    "    max_scores, max_idx_class = out.max(\n",
    "        dim=1)\n",
    "    return (max_idx_class == lbl).sum().item() / n\n",
    "\n",
    "\n",
    "model = MNISTNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-8)\n",
    "\n",
    "\n",
    "def load_model(path):\n",
    "    load_dict = torch.load(path)\n",
    "    model.load_state_dict(load_dict['model_state_dict'])\n",
    "    optimizer.load_state_dict(load_dict['optimizer_state_dict'])\n",
    "\n",
    "\n",
    "if model_load:\n",
    "    load_model(\"runs/run Jul-04-2023 23:27:12/checkpoint Jul-05-2023 01:19:35.pt\")\n",
    "    model.train()\n",
    "\n",
    "train_loss_array = []\n",
    "train_accuracy_array = []\n",
    "validation_loss_array = []\n",
    "validation_accuracy_array = []\n",
    "epoch_array = []\n",
    "total_loss = 0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-21T23:55:05.276095Z",
     "start_time": "2023-07-21T23:55:05.236788Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training the CNN\n",
    "\n",
    "Now that we have created our model, we will focus on the training and validation loops. Let's first start with the training loop. We first use <code>optimizer.zero_grad()</code> in order to clear our gradients for an accurate parameter updates. Without this, the model will not train properly. We then take the output from the model and put it into the loss function we call <code>criterion()</code> in order to get the loss. We print it out and add it to our total loss gathered. We then use the <code>backward()</code> function to calculate the gradient in the backward pass, and we take a step with the optimizer. This happens 64 times due to our batch size, and after printing out some results for the training phase, we move on to validation. We do the same thing, however we do not perform a backward pass, nor do we take steps in the optimizer, as we do not wish to change the model during this phase. We save the checkpoint and repeat this however many times is specified by the <code>epochs</code> variable. We then prompt the user to select the best directory to put into testing."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - Epoch: 0 Loss: 0.0026618908159434795\n",
      "Training - Epoch: 0 Loss: 0.008727557957172394\n",
      "Training - Epoch: 0 Loss: 0.007648566272109747\n",
      "Training - Epoch: 0 Loss: 0.0014483779668807983\n",
      "Training - Epoch: 0 Loss: 0.022248005494475365\n",
      "Training - Epoch: 0 Loss: 0.0016599394148215652\n",
      "Training - Epoch: 0 Loss: 0.011963305063545704\n",
      "Training - Epoch: 0 Loss: 0.002422294346615672\n",
      "Training - Epoch: 0 Loss: 0.004159153904765844\n",
      "Training - Epoch: 0 Loss: 0.0001549858134239912\n",
      "Training - Epoch: 0 Loss: 0.00043410094804130495\n",
      "Training - Epoch: 0 Loss: 0.00933658704161644\n",
      "Training - Epoch: 0 Loss: 0.005613958463072777\n",
      "Training - Epoch: 0 Loss: 0.001738066552206874\n",
      "Training - Epoch: 0 Loss: 0.0016440398758277297\n",
      "Training - Epoch: 0 Loss: 0.003450829302892089\n",
      "Training - Epoch: 0 Loss: 0.000993796973489225\n",
      "Training - Epoch: 0 Loss: 0.0013165655545890331\n",
      "Training - Epoch: 0 Loss: 0.010290000587701797\n",
      "Training - Epoch: 0 Loss: 0.005817316006869078\n",
      "Training - Epoch: 0 Loss: 0.006770985666662455\n",
      "Training - Epoch: 0 Loss: 0.021442366763949394\n",
      "Training - Epoch: 0 Loss: 0.0024902760051190853\n",
      "Training - Epoch: 0 Loss: 0.005526773631572723\n",
      "Training - Epoch: 0 Loss: 0.004174097441136837\n",
      "Training - Epoch: 0 Loss: 0.0028551737777888775\n",
      "Training - Epoch: 0 Loss: 0.002332212869077921\n",
      "Training - Epoch: 0 Loss: 0.004728511441498995\n",
      "Training - Epoch: 0 Loss: 0.01661040633916855\n",
      "Training - Epoch: 0 Loss: 0.0025052998680621386\n",
      "Training - Epoch: 0 Loss: 0.0013878962490707636\n",
      "Training - Epoch: 0 Loss: 0.014146876521408558\n",
      "Training - Epoch: 0 Loss: 0.0026181568391621113\n",
      "Training - Epoch: 0 Loss: 0.004436834249645472\n",
      "Training - Epoch: 0 Loss: 0.0004639117978513241\n",
      "Training - Epoch: 0 Loss: 0.001451708609238267\n",
      "Training - Epoch: 0 Loss: 0.08523771911859512\n",
      "Training - Epoch: 0 Loss: 0.004190964624285698\n",
      "Training - Epoch: 0 Loss: 0.008913378231227398\n",
      "Training - Epoch: 0 Loss: 0.0010450095869600773\n",
      "Training - Epoch: 0 Loss: 0.012340500950813293\n",
      "Training - Epoch: 0 Loss: 0.0029920078814029694\n",
      "Training - Epoch: 0 Loss: 0.0004953710595145822\n",
      "Training - Epoch: 0 Loss: 0.014048803597688675\n",
      "Training - Epoch: 0 Loss: 0.026224087923765182\n",
      "Training - Epoch: 0 Loss: 0.02014465071260929\n",
      "Training - Epoch: 0 Loss: 0.0708606094121933\n",
      "Training - Epoch: 0 Loss: 0.004477336071431637\n",
      "Training - Epoch: 0 Loss: 0.0029999606776982546\n",
      "Training - Epoch: 0 Loss: 0.011782364919781685\n",
      "Training - Epoch: 0 Loss: 0.06075606122612953\n",
      "Training - Epoch: 0 Loss: 0.0091242715716362\n",
      "Training - Epoch: 0 Loss: 0.002319368766620755\n",
      "Training - Epoch: 0 Loss: 0.0006864232127554715\n",
      "Training - Epoch: 0 Loss: 0.0022959550842642784\n",
      "Training - Epoch: 0 Loss: 0.0022527785040438175\n",
      "Training - Epoch: 0 Loss: 0.0008389423601329327\n",
      "Training - Epoch: 0 Loss: 0.002351234434172511\n",
      "Training - Epoch: 0 Loss: 0.002429723972454667\n",
      "Training - Epoch: 0 Loss: 0.0006714159972034395\n",
      "Training - Epoch: 0 Loss: 0.010920580476522446\n",
      "Training - Epoch: 0 Loss: 0.004163170233368874\n",
      "Training - Epoch: 0 Loss: 0.009800510480999947\n",
      "Training - Epoch: 0 Loss: 0.09333424270153046\n",
      "Training - Epoch: 0 Loss: 0.0034065982326865196\n",
      "Training - Epoch: 0 Loss: 0.0037407204508781433\n",
      "Training - Epoch: 0 Loss: 0.002179550938308239\n",
      "Training - Epoch: 0 Loss: 0.018224114552140236\n",
      "Training - Epoch: 0 Loss: 0.001729978364892304\n",
      "Training - Epoch: 0 Loss: 0.016903502866625786\n",
      "Training - Epoch: 0 Loss: 0.011141502298414707\n",
      "Training - Epoch: 0 Loss: 0.0030509501229971647\n",
      "Training - Epoch: 0 Loss: 0.002411486580967903\n",
      "Training - Epoch: 0 Loss: 0.0017299437895417213\n",
      "Training - Epoch: 0 Loss: 0.0249464213848114\n",
      "Training - Epoch: 0 Loss: 0.0010292540537193418\n",
      "Training - Epoch: 0 Loss: 0.0015569707611575723\n",
      "Training - Epoch: 0 Loss: 0.03395640850067139\n",
      "Training - Epoch: 0 Loss: 0.016458194702863693\n",
      "Training - Epoch: 0 Loss: 0.0013218045933172107\n",
      "Training - Epoch: 0 Loss: 0.0002703400095924735\n",
      "Training - Epoch: 0 Loss: 0.0030496122781187296\n",
      "Training - Epoch: 0 Loss: 0.0044695474207401276\n",
      "Training - Epoch: 0 Loss: 0.0022271976340562105\n",
      "Training - Epoch: 0 Loss: 0.000369230197975412\n",
      "Training - Epoch: 0 Loss: 0.027932988479733467\n",
      "Training - Epoch: 0 Loss: 0.018635563552379608\n",
      "Training - Epoch: 0 Loss: 0.004634417127817869\n",
      "Training - Epoch: 0 Loss: 0.06716682016849518\n",
      "Training - Epoch: 0 Loss: 0.0247518140822649\n",
      "Training - Epoch: 0 Loss: 0.0017095232615247369\n",
      "Training - Epoch: 0 Loss: 0.0010224264115095139\n",
      "Training - Epoch: 0 Loss: 0.02732117846608162\n",
      "Training - Epoch: 0 Loss: 0.03934156149625778\n",
      "Training - Epoch: 0 Loss: 0.0021201008930802345\n",
      "Training - Epoch: 0 Loss: 0.019638167694211006\n",
      "Training - Epoch: 0 Loss: 0.0005098305991850793\n",
      "Training - Epoch: 0 Loss: 0.02626081556081772\n",
      "Training - Epoch: 0 Loss: 0.00150818913243711\n",
      "Training - Epoch: 0 Loss: 0.0023257816210389137\n",
      "Training - Epoch: 0 Loss: 0.02034388855099678\n",
      "Training - Epoch: 0 Loss: 0.010626744478940964\n",
      "Training - Epoch: 0 Loss: 0.01999524235725403\n",
      "Training - Epoch: 0 Loss: 0.013795231468975544\n",
      "Training - Epoch: 0 Loss: 0.0048872740007936954\n",
      "Training - Epoch: 0 Loss: 0.0073990910314023495\n",
      "Training - Epoch: 0 Loss: 0.0030235410667955875\n",
      "Training - Epoch: 0 Loss: 0.0071514807641506195\n",
      "Training - Epoch: 0 Loss: 0.0017799937631934881\n",
      "Training - Epoch: 0 Loss: 0.011968179605901241\n",
      "Training - Epoch: 0 Loss: 0.00035346407094039023\n",
      "Training - Epoch: 0 Loss: 0.002742503769695759\n",
      "Training - Epoch: 0 Loss: 0.08098262548446655\n",
      "Training - Epoch: 0 Loss: 0.032281938940286636\n",
      "Training - Epoch: 0 Loss: 0.003176257945597172\n",
      "Training - Epoch: 0 Loss: 0.007644574157893658\n",
      "Training - Epoch: 0 Loss: 0.00912531092762947\n",
      "Training - Epoch: 0 Loss: 0.02082253061234951\n",
      "Training - Epoch: 0 Loss: 0.03049894981086254\n",
      "Training - Epoch: 0 Loss: 0.0017989741172641516\n",
      "Training - Epoch: 0 Loss: 0.09158122539520264\n",
      "Training - Epoch: 0 Loss: 0.0033802518155425787\n",
      "Training - Epoch: 0 Loss: 0.01305342186242342\n",
      "Training - Epoch: 0 Loss: 0.005805128253996372\n",
      "Training - Epoch: 0 Loss: 0.006094193086028099\n",
      "Training - Epoch: 0 Loss: 0.008392897434532642\n",
      "Training - Epoch: 0 Loss: 0.002631933195516467\n",
      "Training - Epoch: 0 Loss: 0.008937538601458073\n",
      "Training - Epoch: 0 Loss: 0.005773188546299934\n",
      "Training - Epoch: 0 Loss: 0.007738059386610985\n",
      "Training - Epoch: 0 Loss: 0.0011244900524616241\n",
      "Training - Epoch: 0 Loss: 0.002317119622603059\n",
      "Training - Epoch: 0 Loss: 0.0015674694441258907\n",
      "Training - Epoch: 0 Loss: 0.019303807988762856\n",
      "Training - Epoch: 0 Loss: 0.0021990605164319277\n",
      "Training - Epoch: 0 Loss: 0.003970568999648094\n",
      "Training - Epoch: 0 Loss: 0.009934642352163792\n",
      "Training - Epoch: 0 Loss: 0.006713468115776777\n",
      "Training - Epoch: 0 Loss: 0.0015302832471206784\n",
      "Training - Epoch: 0 Loss: 0.002587272319942713\n",
      "Training - Epoch: 0 Loss: 0.029682455584406853\n",
      "Training - Epoch: 0 Loss: 0.0007939127390272915\n",
      "Training - Epoch: 0 Loss: 0.07028817385435104\n",
      "Training - Epoch: 0 Loss: 0.005396021530032158\n",
      "Training - Epoch: 0 Loss: 0.004158823750913143\n",
      "Training - Epoch: 0 Loss: 0.0006558411405421793\n",
      "Training - Epoch: 0 Loss: 0.002922078361734748\n",
      "Training - Epoch: 0 Loss: 0.009506511501967907\n",
      "Training - Epoch: 0 Loss: 0.0038730392698198557\n",
      "Training - Epoch: 0 Loss: 0.0017039311351254582\n",
      "Training - Epoch: 0 Loss: 0.00154621503315866\n",
      "Training - Epoch: 0 Loss: 0.0012000649003311992\n",
      "Training - Epoch: 0 Loss: 0.006491092965006828\n",
      "Training - Epoch: 0 Loss: 0.002056503901258111\n",
      "Training - Epoch: 0 Loss: 0.007619061972945929\n",
      "Training - Epoch: 0 Loss: 0.0029171656351536512\n",
      "Training - Epoch: 0 Loss: 0.00670479005202651\n",
      "Training - Epoch: 0 Loss: 0.0253895316272974\n",
      "Training - Epoch: 0 Loss: 0.0015780593967065215\n",
      "Training - Epoch: 0 Loss: 0.0060096257366240025\n",
      "Training - Epoch: 0 Loss: 0.002670563291758299\n",
      "Training - Epoch: 0 Loss: 0.007145193871110678\n",
      "Training - Epoch: 0 Loss: 0.009260599501430988\n",
      "Training - Epoch: 0 Loss: 0.00021618398022837937\n",
      "Training - Epoch: 0 Loss: 0.0002269847027491778\n",
      "Training - Epoch: 0 Loss: 0.004982150625437498\n",
      "Training - Epoch: 0 Loss: 0.0023260668385773897\n",
      "Training - Epoch: 0 Loss: 0.10807038843631744\n",
      "Training - Epoch: 0 Loss: 0.006094995886087418\n",
      "Training - Epoch: 0 Loss: 0.017039010301232338\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 11\u001B[0m\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTraining - Epoch: \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(epoch) \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m Loss: \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(loss\u001B[38;5;241m.\u001B[39mitem()))\n\u001B[1;32m     10\u001B[0m     total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n\u001B[0;32m---> 11\u001B[0m     \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     12\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTotal Train loss on epoch \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(epoch) \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m: \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(total_loss))\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/neuraltest/lib/python3.10/site-packages/torch/_tensor.py:396\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    387\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    388\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    389\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    390\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    394\u001B[0m         create_graph\u001B[38;5;241m=\u001B[39mcreate_graph,\n\u001B[1;32m    395\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs)\n\u001B[0;32m--> 396\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/neuraltest/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    168\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    170\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[1;32m    171\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    172\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 173\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    174\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    175\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    temp_accuracy_array = []\n",
    "    for inputs, labels in mnist_train:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        temp_accuracy_array.append(get_accuracy(output, labels))\n",
    "        loss = criterion(output, labels)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_accuracy_array.append(sum(temp_accuracy_array)/len(temp_accuracy_array))\n",
    "    temp_accuracy_array = []\n",
    "\n",
    "    print(\"Total Train loss on epoch \" + str(epoch) + \": \" + str(total_loss))\n",
    "    train_loss_array.append(total_loss)\n",
    "    total_loss = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in mnist_validation:\n",
    "            output = model(inputs)\n",
    "            temp_accuracy_array.append(get_accuracy(output, labels))\n",
    "            loss = criterion(output, labels)\n",
    "            total_loss += loss.item()\n",
    "    validation_accuracy_array.append(sum(temp_accuracy_array)/len(temp_accuracy_array))\n",
    "\n",
    "    print(\"Total validation loss on epoch \" + str(epoch) + \": \" + str(total_loss))\n",
    "    validation_loss_array.append(total_loss)\n",
    "    total_loss = 0\n",
    "    epoch_array.append(epoch)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    checkpoint = {'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict(),\n",
    "                  'scheduler_state_dict': scheduler.state_dict()}\n",
    "    save_checkpoint(checkpoint)\n",
    "\n",
    "load_model(str(input(\"Directory for best epoch: \")))\n",
    "model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-21T23:55:21.693879Z",
     "start_time": "2023-07-21T23:55:05.265662Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Results\n",
    "\n",
    "We finish by adding an array that keeps track of the accuracy during testing, and we print out the results and finally plot the losses and accuracies for training and validation.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_accuracy_array = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in mnist_test:\n",
    "        outputs = model(inputs)\n",
    "        num_correct = 0\n",
    "        test_accuracy_array.append(get_accuracy(outputs, labels))\n",
    "\n",
    "print(\"Final accuracy: \" + str(sum(test_accuracy_array) / len(test_accuracy_array)))\n",
    "\n",
    "ax1.plot(epoch_array, train_loss_array, label='Training Loss', linestyle='dashed', color='red')\n",
    "ax1.plot(epoch_array, validation_loss_array, label='Validation Loss', linestyle='dotted', color='blue')\n",
    "ax1.legend()\n",
    "ax1.set_title('Losses by Epoch')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Losses')\n",
    "\n",
    "ax2.plot(epoch_array, train_accuracy_array, label='Training Accuracy', linestyle='dashed', color='red')\n",
    "ax2.plot(epoch_array, validation_accuracy_array, label='Validation Accuracy', linestyle='dotted', color='blue')\n",
    "ax2.legend()\n",
    "ax2.set_title('Accuracy by Epoch')\n",
    "ax2.set_xlabel('Epochs')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.ylim(0, 1)\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
